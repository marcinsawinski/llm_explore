{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import wandb\n",
    "import re\n",
    "api = wandb.Api()\n",
    "r = []\n",
    "def read(r):\n",
    "    for run in runs: \n",
    "        # .summary contains the output keys/values for metrics like accuracy.\n",
    "        #  We call ._json_dict to omit large files \n",
    "        summary_list.append(run.summary._json_dict)\n",
    "\n",
    "        # .config contains the hyperparameters.\n",
    "        #  We remove special values that start with _.\n",
    "        config_list.append(\n",
    "            {k: v for k,v in run.config.items()\n",
    "            if not k.startswith('_')})\n",
    "\n",
    "        # .name is the human-readable name of the run.\n",
    "        name_list.append(run.name)\n",
    "\n",
    "        metadata_list.append(run.metadata)\n",
    "\n",
    "        hist_list.append(run.history())\n",
    "\n",
    "        system_metrics = run.history(stream=\"events\")\n",
    "        allocated = sorted([x for x in system_metrics.columns if re.match('system\\.gpu\\..\\.memoryAllocated$', x)])\n",
    "        allocated_bytes = sorted([x for x in system_metrics.columns if re.match('system\\.gpu\\..\\.memoryAllocatedBytes$', x)])\n",
    "        system_metrics['allocatedgb'] = round(system_metrics[allocated_bytes].sum(axis=1)*1e-09,2)\n",
    "        system_metrics['allocated'] = round(system_metrics[allocated].mean(axis=1))\n",
    "        gpus = {'allocatedmax':system_metrics['allocated'].max(),\n",
    "        'allocatedmed':system_metrics['allocated'].median(),\n",
    "        'allocatedgbmax':system_metrics['allocatedgb'].max(),\n",
    "        'allocatedgbmed':system_metrics['allocatedgb'].median()}\n",
    "        sys_list.append(gpus)\n",
    "\n",
    "# Project is specified by <entity/project-name>\n",
    "summary_list, config_list, name_list, metadata_list, hist_list, sys_list = [], [], [], [],[],[]\n",
    "runs = api.runs(\"openfact/clef23_p4_llama\")\n",
    "read(runs)\n",
    "runs = api.runs(\"checkw/clef23_p5_llama_floki\")\n",
    "read(runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_df = pd.DataFrame({\n",
    "    \"summary\": summary_list,\n",
    "    \"config\": config_list,\n",
    "    \"name\": name_list,\n",
    "    \"metadata\": metadata_list,\n",
    "    \"sys\":sys_list,\n",
    "    })\n",
    "\n",
    "runs_df.to_csv(\"project.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>model</th>\n",
       "      <th>o</th>\n",
       "      <th>dataset</th>\n",
       "      <th>rand</th>\n",
       "      <th>os</th>\n",
       "      <th>python</th>\n",
       "      <th>heartbeatAt</th>\n",
       "      <th>startedAt</th>\n",
       "      <th>docker</th>\n",
       "      <th>...</th>\n",
       "      <th>train/train_runtime</th>\n",
       "      <th>eval/samples_per_second</th>\n",
       "      <th>train/train_samples_per_second</th>\n",
       "      <th>train/total_flos</th>\n",
       "      <th>train/learning_rate</th>\n",
       "      <th>_wandb.runtime</th>\n",
       "      <th>allocatedmax</th>\n",
       "      <th>allocatedmed</th>\n",
       "      <th>allocatedgbmax</th>\n",
       "      <th>allocatedgbmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama13|adamw_torch 8 1e-05 f1_bi 2|r21|0</td>\n",
       "      <td>llama13</td>\n",
       "      <td>adamw_torch 8 1e-05 f1_bi 2</td>\n",
       "      <td>r21</td>\n",
       "      <td>0</td>\n",
       "      <td>Linux-5.10.0-21-amd64-x86_64-with-glibc2.29</td>\n",
       "      <td>3.8.10</td>\n",
       "      <td>2023-11-16T01:47:01.388901</td>\n",
       "      <td>2023-11-16T01:47:00.633252</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>22680.5925</td>\n",
       "      <td>6.397</td>\n",
       "      <td>0.678</td>\n",
       "      <td>1.312225e+17</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>23040.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>19.14</td>\n",
       "      <td>18.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama7|paged_adamw_8bit 8 1e-05 f1_bi 2|r21|0</td>\n",
       "      <td>llama7</td>\n",
       "      <td>paged_adamw_8bit 8 1e-05 f1_bi 2</td>\n",
       "      <td>r21</td>\n",
       "      <td>0</td>\n",
       "      <td>Linux-5.10.0-21-amd64-x86_64-with-glibc2.29</td>\n",
       "      <td>3.8.10</td>\n",
       "      <td>2023-11-15T20:38:35.695311</td>\n",
       "      <td>2023-11-15T20:38:34.820752</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>18209.3552</td>\n",
       "      <td>11.445</td>\n",
       "      <td>0.845</td>\n",
       "      <td>9.511740e+16</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>18418.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>12.55</td>\n",
       "      <td>12.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 270 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            name    model  \\\n",
       "0      llama13|adamw_torch 8 1e-05 f1_bi 2|r21|0  llama13   \n",
       "1  llama7|paged_adamw_8bit 8 1e-05 f1_bi 2|r21|0   llama7   \n",
       "\n",
       "                                  o dataset rand  \\\n",
       "0       adamw_torch 8 1e-05 f1_bi 2     r21    0   \n",
       "1  paged_adamw_8bit 8 1e-05 f1_bi 2     r21    0   \n",
       "\n",
       "                                            os  python  \\\n",
       "0  Linux-5.10.0-21-amd64-x86_64-with-glibc2.29  3.8.10   \n",
       "1  Linux-5.10.0-21-amd64-x86_64-with-glibc2.29  3.8.10   \n",
       "\n",
       "                  heartbeatAt                   startedAt docker  ...  \\\n",
       "0  2023-11-16T01:47:01.388901  2023-11-16T01:47:00.633252   None  ...   \n",
       "1  2023-11-15T20:38:35.695311  2023-11-15T20:38:34.820752   None  ...   \n",
       "\n",
       "  train/train_runtime eval/samples_per_second train/train_samples_per_second  \\\n",
       "0          22680.5925                   6.397                          0.678   \n",
       "1          18209.3552                  11.445                          0.845   \n",
       "\n",
       "  train/total_flos train/learning_rate _wandb.runtime allocatedmax  \\\n",
       "0     1.312225e+17            0.000004        23040.0         41.0   \n",
       "1     9.511740e+16            0.000002        18418.0         27.0   \n",
       "\n",
       "  allocatedmed allocatedgbmax allocatedgbmed  \n",
       "0         40.0          19.14          18.89  \n",
       "1         26.0          12.55          12.40  \n",
       "\n",
       "[2 rows x 270 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = [runs_df[['name']],\n",
    "        pd.DataFrame(runs_df.name.str.split('|').to_list(), columns=['model','o','dataset','rand']),\n",
    "        pd.json_normalize(runs_df.metadata),\n",
    "        pd.json_normalize(runs_df.config),\n",
    "        pd.json_normalize(runs_df.summary),\n",
    "        pd.json_normalize(runs_df.sys),\n",
    "]\n",
    "data = pd.concat(dfs, axis=1)\n",
    "data.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>llama13|adamw_torch 8 1e-05 f1_bi 2|r21|0</td>\n",
       "      <td>llama7|paged_adamw_8bit 8 1e-05 f1_bi 2|r21|0</td>\n",
       "      <td>llama70|paged_adamw_8bit 8 1e-05 f1_bi 2|r21|0</td>\n",
       "      <td>llama7|paged_adamw_8bit 8 1e-05 f1_bi 2|r21|0</td>\n",
       "      <td>llama13|paged_adamw_8bit 8 1e-05 f1_bi 2|r21|0</td>\n",
       "      <td>llama7|adamw_torch 8 1e-05 f1_bi 2|r21|0</td>\n",
       "      <td>llama7|adamw_torch 8 1e-05 f1_bi 2|r21|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>llama13</td>\n",
       "      <td>llama7</td>\n",
       "      <td>llama70</td>\n",
       "      <td>llama7</td>\n",
       "      <td>llama13</td>\n",
       "      <td>llama7</td>\n",
       "      <td>llama7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host</th>\n",
       "      <td>c301b88b57d6</td>\n",
       "      <td>c301b88b57d6</td>\n",
       "      <td>clef-l4-8</td>\n",
       "      <td>clef-l4-4</td>\n",
       "      <td>c301b88b57d6</td>\n",
       "      <td>clef-l4-4</td>\n",
       "      <td>c301b88b57d6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <td>r21</td>\n",
       "      <td>r21</td>\n",
       "      <td>r21</td>\n",
       "      <td>r21</td>\n",
       "      <td>r21</td>\n",
       "      <td>r21</td>\n",
       "      <td>r21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codePath</th>\n",
       "      <td>cp_train_template_llama.py</td>\n",
       "      <td>cp_train_template_llama.py</td>\n",
       "      <td>cp_train_template_llama.py</td>\n",
       "      <td>cp_train_template_llama.py</td>\n",
       "      <td>cp_train_template_llama.py</td>\n",
       "      <td>cp_train_template_llama.py</td>\n",
       "      <td>cp_train_template_llama.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hw</th>\n",
       "      <td>NVIDIA GeForce RTX 2080 Ti x 4</td>\n",
       "      <td>NVIDIA GeForce RTX 2080 Ti x 4</td>\n",
       "      <td>NVIDIA L4 x 8</td>\n",
       "      <td>NVIDIA L4 x 4</td>\n",
       "      <td>NVIDIA GeForce RTX 2080 Ti x 4</td>\n",
       "      <td>NVIDIA L4 x 4</td>\n",
       "      <td>NVIDIA GeForce RTX 2080 Ti x 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_type</th>\n",
       "      <td>llama</td>\n",
       "      <td>llama</td>\n",
       "      <td>llama</td>\n",
       "      <td>llama</td>\n",
       "      <td>llama</td>\n",
       "      <td>llama</td>\n",
       "      <td>llama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric_for_best_model</th>\n",
       "      <td>f1_bi</td>\n",
       "      <td>f1_bi</td>\n",
       "      <td>f1_bi</td>\n",
       "      <td>f1_bi</td>\n",
       "      <td>f1_bi</td>\n",
       "      <td>f1_bi</td>\n",
       "      <td>f1_bi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optim</th>\n",
       "      <td>adamw_torch</td>\n",
       "      <td>paged_adamw_8bit</td>\n",
       "      <td>paged_adamw_8bit</td>\n",
       "      <td>paged_adamw_8bit</td>\n",
       "      <td>paged_adamw_8bit</td>\n",
       "      <td>adamw_torch</td>\n",
       "      <td>adamw_torch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>384.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_runtime</th>\n",
       "      <td>23041.139829</td>\n",
       "      <td>18419.353086</td>\n",
       "      <td>90920.313469</td>\n",
       "      <td>28221.114386</td>\n",
       "      <td>23665.785077</td>\n",
       "      <td>16658.944845</td>\n",
       "      <td>13044.447381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time(h)</th>\n",
       "      <td>6.4</td>\n",
       "      <td>5.12</td>\n",
       "      <td>25.26</td>\n",
       "      <td>7.84</td>\n",
       "      <td>6.57</td>\n",
       "      <td>4.63</td>\n",
       "      <td>3.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allocatedmax</th>\n",
       "      <td>41.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allocatedmed</th>\n",
       "      <td>40.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allocatedgbmax</th>\n",
       "      <td>19.14</td>\n",
       "      <td>12.55</td>\n",
       "      <td>85.0</td>\n",
       "      <td>16.92</td>\n",
       "      <td>18.14</td>\n",
       "      <td>16.7</td>\n",
       "      <td>12.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allocatedgbmed</th>\n",
       "      <td>18.89</td>\n",
       "      <td>12.4</td>\n",
       "      <td>85.0</td>\n",
       "      <td>16.79</td>\n",
       "      <td>17.89</td>\n",
       "      <td>16.7</td>\n",
       "      <td>12.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train/train_runtime</th>\n",
       "      <td>22680.5925</td>\n",
       "      <td>18209.3552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27931.2268</td>\n",
       "      <td>23304.7849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12834.5342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval/runtime</th>\n",
       "      <td>156.3141</td>\n",
       "      <td>87.3775</td>\n",
       "      <td>1101.8271</td>\n",
       "      <td>116.5827</td>\n",
       "      <td>156.1842</td>\n",
       "      <td>116.3654</td>\n",
       "      <td>87.3016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train/global_step</th>\n",
       "      <td>1100</td>\n",
       "      <td>1560</td>\n",
       "      <td>640</td>\n",
       "      <td>1810</td>\n",
       "      <td>1130</td>\n",
       "      <td>1090</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train/total_flos</th>\n",
       "      <td>131222477432217600.0</td>\n",
       "      <td>95117398319923200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110365458326323200.0</td>\n",
       "      <td>134802899735961600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67060967908147200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train/train_steps_per_second</th>\n",
       "      <td>0.085</td>\n",
       "      <td>0.106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train/train_samples_per_second</th>\n",
       "      <td>0.678</td>\n",
       "      <td>0.845</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval/steps_per_second</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1.431</td>\n",
       "      <td>0.113</td>\n",
       "      <td>1.072</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.074</td>\n",
       "      <td>1.432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval/samples_per_second</th>\n",
       "      <td>6.397</td>\n",
       "      <td>11.445</td>\n",
       "      <td>0.908</td>\n",
       "      <td>8.578</td>\n",
       "      <td>6.403</td>\n",
       "      <td>8.594</td>\n",
       "      <td>11.455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train/train_loss</th>\n",
       "      <td>0.29443</td>\n",
       "      <td>0.352834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.34167</td>\n",
       "      <td>0.289484</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.407498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train/loss</th>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.3721</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0474</td>\n",
       "      <td>0.2473</td>\n",
       "      <td>0.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval/loss</th>\n",
       "      <td>0.310096</td>\n",
       "      <td>0.34976</td>\n",
       "      <td>0.326144</td>\n",
       "      <td>0.335258</td>\n",
       "      <td>0.308528</td>\n",
       "      <td>0.34023</td>\n",
       "      <td>0.328119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval/f1_bi</th>\n",
       "      <td>0.865225</td>\n",
       "      <td>0.860927</td>\n",
       "      <td>0.843077</td>\n",
       "      <td>0.868167</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.861953</td>\n",
       "      <td>0.863866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval/f1_ma</th>\n",
       "      <td>0.903663</td>\n",
       "      <td>0.900378</td>\n",
       "      <td>0.883761</td>\n",
       "      <td>0.90433</td>\n",
       "      <td>0.903845</td>\n",
       "      <td>0.901816</td>\n",
       "      <td>0.903107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval/prec</th>\n",
       "      <td>0.921986</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.827795</td>\n",
       "      <td>0.891089</td>\n",
       "      <td>0.919014</td>\n",
       "      <td>0.930909</td>\n",
       "      <td>0.931159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval/rec</th>\n",
       "      <td>0.815047</td>\n",
       "      <td>0.815047</td>\n",
       "      <td>0.858934</td>\n",
       "      <td>0.846395</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.802508</td>\n",
       "      <td>0.805643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval/acc</th>\n",
       "      <td>0.919</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.814</td>\n",
       "      <td>0.834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.919</td>\n",
       "      <td>0.912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.731</td>\n",
       "      <td>0.769</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.741</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_accuracy</th>\n",
       "      <td>0.887</td>\n",
       "      <td>0.896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dev_test_f1</th>\n",
       "      <td>0.758</td>\n",
       "      <td>0.793</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dev_test_precision</th>\n",
       "      <td>0.937</td>\n",
       "      <td>0.925</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.941</td>\n",
       "      <td>0.932</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dev_test_recall</th>\n",
       "      <td>0.637</td>\n",
       "      <td>0.694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dev_test_accuracy</th>\n",
       "      <td>0.797</td>\n",
       "      <td>0.819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        0  \\\n",
       "name                            llama13|adamw_torch 8 1e-05 f1_bi 2|r21|0   \n",
       "model                                                             llama13   \n",
       "host                                                         c301b88b57d6   \n",
       "dataset                                                               r21   \n",
       "codePath                                       cp_train_template_llama.py   \n",
       "hw                                         NVIDIA GeForce RTX 2080 Ti x 4   \n",
       "model_type                                                          llama   \n",
       "metric_for_best_model                                               f1_bi   \n",
       "optim                                                         adamw_torch   \n",
       "s                                                                   384.0   \n",
       "_runtime                                                     23041.139829   \n",
       "time(h)                                                               6.4   \n",
       "allocatedmax                                                         41.0   \n",
       "allocatedmed                                                         40.0   \n",
       "allocatedgbmax                                                      19.14   \n",
       "allocatedgbmed                                                      18.89   \n",
       "train/train_runtime                                            22680.5925   \n",
       "eval/runtime                                                     156.3141   \n",
       "train/global_step                                                    1100   \n",
       "train/total_flos                                     131222477432217600.0   \n",
       "train/train_steps_per_second                                        0.085   \n",
       "train/train_samples_per_second                                      0.678   \n",
       "eval/steps_per_second                                                 0.8   \n",
       "eval/samples_per_second                                             6.397   \n",
       "train/train_loss                                                  0.29443   \n",
       "train/loss                                                         0.0985   \n",
       "eval/loss                                                        0.310096   \n",
       "eval/f1_bi                                                       0.865225   \n",
       "eval/f1_ma                                                       0.903663   \n",
       "eval/prec                                                        0.921986   \n",
       "eval/rec                                                         0.815047   \n",
       "eval/acc                                                            0.919   \n",
       "test_f1                                                             0.814   \n",
       "test_precision                                                      0.919   \n",
       "test_recall                                                         0.731   \n",
       "test_accuracy                                                       0.887   \n",
       "dev_test_f1                                                         0.758   \n",
       "dev_test_precision                                                  0.937   \n",
       "dev_test_recall                                                     0.637   \n",
       "dev_test_accuracy                                                   0.797   \n",
       "\n",
       "                                                                            1  \\\n",
       "name                            llama7|paged_adamw_8bit 8 1e-05 f1_bi 2|r21|0   \n",
       "model                                                                  llama7   \n",
       "host                                                             c301b88b57d6   \n",
       "dataset                                                                   r21   \n",
       "codePath                                           cp_train_template_llama.py   \n",
       "hw                                             NVIDIA GeForce RTX 2080 Ti x 4   \n",
       "model_type                                                              llama   \n",
       "metric_for_best_model                                                   f1_bi   \n",
       "optim                                                        paged_adamw_8bit   \n",
       "s                                                                       307.0   \n",
       "_runtime                                                         18419.353086   \n",
       "time(h)                                                                  5.12   \n",
       "allocatedmax                                                             27.0   \n",
       "allocatedmed                                                             26.0   \n",
       "allocatedgbmax                                                          12.55   \n",
       "allocatedgbmed                                                           12.4   \n",
       "train/train_runtime                                                18209.3552   \n",
       "eval/runtime                                                          87.3775   \n",
       "train/global_step                                                        1560   \n",
       "train/total_flos                                          95117398319923200.0   \n",
       "train/train_steps_per_second                                            0.106   \n",
       "train/train_samples_per_second                                          0.845   \n",
       "eval/steps_per_second                                                   1.431   \n",
       "eval/samples_per_second                                                11.445   \n",
       "train/train_loss                                                     0.352834   \n",
       "train/loss                                                             0.0345   \n",
       "eval/loss                                                             0.34976   \n",
       "eval/f1_bi                                                           0.860927   \n",
       "eval/f1_ma                                                           0.900378   \n",
       "eval/prec                                                            0.912281   \n",
       "eval/rec                                                             0.815047   \n",
       "eval/acc                                                                0.916   \n",
       "test_f1                                                                 0.834   \n",
       "test_precision                                                          0.912   \n",
       "test_recall                                                             0.769   \n",
       "test_accuracy                                                           0.896   \n",
       "dev_test_f1                                                             0.793   \n",
       "dev_test_precision                                                      0.925   \n",
       "dev_test_recall                                                         0.694   \n",
       "dev_test_accuracy                                                       0.819   \n",
       "\n",
       "                                                                             2  \\\n",
       "name                            llama70|paged_adamw_8bit 8 1e-05 f1_bi 2|r21|0   \n",
       "model                                                                  llama70   \n",
       "host                                                                 clef-l4-8   \n",
       "dataset                                                                    r21   \n",
       "codePath                                            cp_train_template_llama.py   \n",
       "hw                                                               NVIDIA L4 x 8   \n",
       "model_type                                                               llama   \n",
       "metric_for_best_model                                                    f1_bi   \n",
       "optim                                                         paged_adamw_8bit   \n",
       "s                                                                       1515.0   \n",
       "_runtime                                                          90920.313469   \n",
       "time(h)                                                                  25.26   \n",
       "allocatedmax                                                              44.0   \n",
       "allocatedmed                                                              44.0   \n",
       "allocatedgbmax                                                            85.0   \n",
       "allocatedgbmed                                                            85.0   \n",
       "train/train_runtime                                                        NaN   \n",
       "eval/runtime                                                         1101.8271   \n",
       "train/global_step                                                          640   \n",
       "train/total_flos                                                           NaN   \n",
       "train/train_steps_per_second                                               NaN   \n",
       "train/train_samples_per_second                                             NaN   \n",
       "eval/steps_per_second                                                    0.113   \n",
       "eval/samples_per_second                                                  0.908   \n",
       "train/train_loss                                                           NaN   \n",
       "train/loss                                                              0.3721   \n",
       "eval/loss                                                             0.326144   \n",
       "eval/f1_bi                                                            0.843077   \n",
       "eval/f1_ma                                                            0.883761   \n",
       "eval/prec                                                             0.827795   \n",
       "eval/rec                                                              0.858934   \n",
       "eval/acc                                                                 0.898   \n",
       "test_f1                                                                    NaN   \n",
       "test_precision                                                             NaN   \n",
       "test_recall                                                                NaN   \n",
       "test_accuracy                                                              NaN   \n",
       "dev_test_f1                                                                NaN   \n",
       "dev_test_precision                                                         NaN   \n",
       "dev_test_recall                                                            NaN   \n",
       "dev_test_accuracy                                                          NaN   \n",
       "\n",
       "                                                                            3  \\\n",
       "name                            llama7|paged_adamw_8bit 8 1e-05 f1_bi 2|r21|0   \n",
       "model                                                                  llama7   \n",
       "host                                                                clef-l4-4   \n",
       "dataset                                                                   r21   \n",
       "codePath                                           cp_train_template_llama.py   \n",
       "hw                                                              NVIDIA L4 x 4   \n",
       "model_type                                                              llama   \n",
       "metric_for_best_model                                                   f1_bi   \n",
       "optim                                                        paged_adamw_8bit   \n",
       "s                                                                       470.0   \n",
       "_runtime                                                         28221.114386   \n",
       "time(h)                                                                  7.84   \n",
       "allocatedmax                                                             18.0   \n",
       "allocatedmed                                                             17.0   \n",
       "allocatedgbmax                                                          16.92   \n",
       "allocatedgbmed                                                          16.79   \n",
       "train/train_runtime                                                27931.2268   \n",
       "eval/runtime                                                         116.5827   \n",
       "train/global_step                                                        1810   \n",
       "train/total_flos                                         110365458326323200.0   \n",
       "train/train_steps_per_second                                            0.069   \n",
       "train/train_samples_per_second                                          0.551   \n",
       "eval/steps_per_second                                                   1.072   \n",
       "eval/samples_per_second                                                 8.578   \n",
       "train/train_loss                                                      0.34167   \n",
       "train/loss                                                             0.0843   \n",
       "eval/loss                                                            0.335258   \n",
       "eval/f1_bi                                                           0.868167   \n",
       "eval/f1_ma                                                            0.90433   \n",
       "eval/prec                                                            0.891089   \n",
       "eval/rec                                                             0.846395   \n",
       "eval/acc                                                                0.918   \n",
       "test_f1                                                                 0.806   \n",
       "test_precision                                                          0.928   \n",
       "test_recall                                                             0.713   \n",
       "test_accuracy                                                           0.884   \n",
       "dev_test_f1                                                              0.75   \n",
       "dev_test_precision                                                      0.941   \n",
       "dev_test_recall                                                         0.623   \n",
       "dev_test_accuracy                                                       0.792   \n",
       "\n",
       "                                                                             4  \\\n",
       "name                            llama13|paged_adamw_8bit 8 1e-05 f1_bi 2|r21|0   \n",
       "model                                                                  llama13   \n",
       "host                                                              c301b88b57d6   \n",
       "dataset                                                                    r21   \n",
       "codePath                                            cp_train_template_llama.py   \n",
       "hw                                              NVIDIA GeForce RTX 2080 Ti x 4   \n",
       "model_type                                                               llama   \n",
       "metric_for_best_model                                                    f1_bi   \n",
       "optim                                                         paged_adamw_8bit   \n",
       "s                                                                        394.0   \n",
       "_runtime                                                          23665.785077   \n",
       "time(h)                                                                   6.57   \n",
       "allocatedmax                                                              38.0   \n",
       "allocatedmed                                                              38.0   \n",
       "allocatedgbmax                                                           18.14   \n",
       "allocatedgbmed                                                           17.89   \n",
       "train/train_runtime                                                 23304.7849   \n",
       "eval/runtime                                                          156.1842   \n",
       "train/global_step                                                         1130   \n",
       "train/total_flos                                          134802899735961600.0   \n",
       "train/train_steps_per_second                                             0.083   \n",
       "train/train_samples_per_second                                            0.66   \n",
       "eval/steps_per_second                                                      0.8   \n",
       "eval/samples_per_second                                                  6.403   \n",
       "train/train_loss                                                      0.289484   \n",
       "train/loss                                                              0.0474   \n",
       "eval/loss                                                             0.308528   \n",
       "eval/f1_bi                                                            0.865672   \n",
       "eval/f1_ma                                                            0.903845   \n",
       "eval/prec                                                             0.919014   \n",
       "eval/rec                                                              0.818182   \n",
       "eval/acc                                                                 0.919   \n",
       "test_f1                                                                  0.821   \n",
       "test_precision                                                            0.92   \n",
       "test_recall                                                              0.741   \n",
       "test_accuracy                                                             0.89   \n",
       "dev_test_f1                                                              0.759   \n",
       "dev_test_precision                                                       0.932   \n",
       "dev_test_recall                                                           0.64   \n",
       "dev_test_accuracy                                                        0.796   \n",
       "\n",
       "                                                                       5  \\\n",
       "name                            llama7|adamw_torch 8 1e-05 f1_bi 2|r21|0   \n",
       "model                                                             llama7   \n",
       "host                                                           clef-l4-4   \n",
       "dataset                                                              r21   \n",
       "codePath                                      cp_train_template_llama.py   \n",
       "hw                                                         NVIDIA L4 x 4   \n",
       "model_type                                                         llama   \n",
       "metric_for_best_model                                              f1_bi   \n",
       "optim                                                        adamw_torch   \n",
       "s                                                                  278.0   \n",
       "_runtime                                                    16658.944845   \n",
       "time(h)                                                             4.63   \n",
       "allocatedmax                                                        17.0   \n",
       "allocatedmed                                                        17.0   \n",
       "allocatedgbmax                                                      16.7   \n",
       "allocatedgbmed                                                      16.7   \n",
       "train/train_runtime                                                  NaN   \n",
       "eval/runtime                                                    116.3654   \n",
       "train/global_step                                                   1090   \n",
       "train/total_flos                                                     NaN   \n",
       "train/train_steps_per_second                                         NaN   \n",
       "train/train_samples_per_second                                       NaN   \n",
       "eval/steps_per_second                                              1.074   \n",
       "eval/samples_per_second                                            8.594   \n",
       "train/train_loss                                                     NaN   \n",
       "train/loss                                                        0.2473   \n",
       "eval/loss                                                        0.34023   \n",
       "eval/f1_bi                                                      0.861953   \n",
       "eval/f1_ma                                                      0.901816   \n",
       "eval/prec                                                       0.930909   \n",
       "eval/rec                                                        0.802508   \n",
       "eval/acc                                                           0.918   \n",
       "test_f1                                                              NaN   \n",
       "test_precision                                                       NaN   \n",
       "test_recall                                                          NaN   \n",
       "test_accuracy                                                        NaN   \n",
       "dev_test_f1                                                          NaN   \n",
       "dev_test_precision                                                   NaN   \n",
       "dev_test_recall                                                      NaN   \n",
       "dev_test_accuracy                                                    NaN   \n",
       "\n",
       "                                                                       6  \n",
       "name                            llama7|adamw_torch 8 1e-05 f1_bi 2|r21|0  \n",
       "model                                                             llama7  \n",
       "host                                                        c301b88b57d6  \n",
       "dataset                                                              r21  \n",
       "codePath                                      cp_train_template_llama.py  \n",
       "hw                                        NVIDIA GeForce RTX 2080 Ti x 4  \n",
       "model_type                                                         llama  \n",
       "metric_for_best_model                                              f1_bi  \n",
       "optim                                                        adamw_torch  \n",
       "s                                                                  217.0  \n",
       "_runtime                                                    13044.447381  \n",
       "time(h)                                                             3.62  \n",
       "allocatedmax                                                        26.0  \n",
       "allocatedmed                                                        26.0  \n",
       "allocatedgbmax                                                     12.31  \n",
       "allocatedgbmed                                                     12.15  \n",
       "train/train_runtime                                           12834.5342  \n",
       "eval/runtime                                                     87.3016  \n",
       "train/global_step                                                   1100  \n",
       "train/total_flos                                     67060967908147200.0  \n",
       "train/train_steps_per_second                                        0.15  \n",
       "train/train_samples_per_second                                     1.199  \n",
       "eval/steps_per_second                                              1.432  \n",
       "eval/samples_per_second                                           11.455  \n",
       "train/train_loss                                                0.407498  \n",
       "train/loss                                                         0.142  \n",
       "eval/loss                                                       0.328119  \n",
       "eval/f1_bi                                                      0.863866  \n",
       "eval/f1_ma                                                      0.903107  \n",
       "eval/prec                                                       0.931159  \n",
       "eval/rec                                                        0.805643  \n",
       "eval/acc                                                           0.919  \n",
       "test_f1                                                            0.812  \n",
       "test_precision                                                     0.899  \n",
       "test_recall                                                        0.741  \n",
       "test_accuracy                                                      0.884  \n",
       "dev_test_f1                                                        0.779  \n",
       "dev_test_precision                                                 0.926  \n",
       "dev_test_recall                                                    0.673  \n",
       "dev_test_accuracy                                                   0.81  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['hw'] = data['gpu']+\" x \"+data['gpu_count'].astype(str)\n",
    "data['s'] = round(data['_runtime']/60)\n",
    "data['time(h)']=round(data['_runtime']/60/60,2)\n",
    "cols = ['name','model','host','dataset','codePath','hw','model_type','metric_for_best_model','optim','s',\n",
    "'_runtime','time(h)',\n",
    "'allocatedmax','allocatedmed','allocatedgbmax','allocatedgbmed',\n",
    "'train/train_runtime','eval/runtime',\n",
    "'train/global_step',\n",
    "'train/total_flos',\n",
    "'train/train_steps_per_second','train/train_samples_per_second',\n",
    "'eval/steps_per_second','eval/samples_per_second',\n",
    "'train/train_loss','train/loss','eval/loss',\n",
    "'eval/f1_bi','eval/f1_ma','eval/prec','eval/rec','eval/acc',\n",
    "'test_f1','test_precision','test_recall','test_accuracy',\n",
    "'dev_test_f1','dev_test_precision','dev_test_recall','dev_test_accuracy'\n",
    "]\n",
    "datat = data[cols].T\n",
    "datat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"15\" halign=\"left\">mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">allocatedgbmax</th>\n",
       "      <th colspan=\"3\" halign=\"left\">allocatedmax</th>\n",
       "      <th colspan=\"3\" halign=\"left\">eval/f1_bi</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_f1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">time(h)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>llama13</th>\n",
       "      <th>llama7</th>\n",
       "      <th>llama70</th>\n",
       "      <th>llama13</th>\n",
       "      <th>llama7</th>\n",
       "      <th>llama70</th>\n",
       "      <th>llama13</th>\n",
       "      <th>llama7</th>\n",
       "      <th>llama70</th>\n",
       "      <th>llama13</th>\n",
       "      <th>llama7</th>\n",
       "      <th>llama70</th>\n",
       "      <th>llama13</th>\n",
       "      <th>llama7</th>\n",
       "      <th>llama70</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>hw</th>\n",
       "      <th>optim</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">r21</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">NVIDIA GeForce RTX 2080 Ti x 4</th>\n",
       "      <th>adamw_torch</th>\n",
       "      <td>19.14</td>\n",
       "      <td>12.31</td>\n",
       "      <td>-</td>\n",
       "      <td>41.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-</td>\n",
       "      <td>0.865225</td>\n",
       "      <td>0.863866</td>\n",
       "      <td>-</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.812</td>\n",
       "      <td>-</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.62</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paged_adamw_8bit</th>\n",
       "      <td>18.14</td>\n",
       "      <td>12.55</td>\n",
       "      <td>-</td>\n",
       "      <td>38.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.860927</td>\n",
       "      <td>-</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.834</td>\n",
       "      <td>-</td>\n",
       "      <td>6.57</td>\n",
       "      <td>5.12</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">NVIDIA L4 x 4</th>\n",
       "      <th>adamw_torch</th>\n",
       "      <td>-</td>\n",
       "      <td>16.7</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.861953</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>4.63</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paged_adamw_8bit</th>\n",
       "      <td>-</td>\n",
       "      <td>16.92</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.868167</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.806</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>7.84</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NVIDIA L4 x 8</th>\n",
       "      <th>paged_adamw_8bit</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>85.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.843077</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>25.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  mean         \\\n",
       "                                                        allocatedgbmax          \n",
       "model                                                          llama13 llama7   \n",
       "dataset hw                             optim                                    \n",
       "r21     NVIDIA GeForce RTX 2080 Ti x 4 adamw_torch               19.14  12.31   \n",
       "                                       paged_adamw_8bit          18.14  12.55   \n",
       "        NVIDIA L4 x 4                  adamw_torch                   -   16.7   \n",
       "                                       paged_adamw_8bit              -  16.92   \n",
       "        NVIDIA L4 x 8                  paged_adamw_8bit              -      -   \n",
       "\n",
       "                                                                              \\\n",
       "                                                                allocatedmax   \n",
       "model                                                   llama70      llama13   \n",
       "dataset hw                             optim                                   \n",
       "r21     NVIDIA GeForce RTX 2080 Ti x 4 adamw_torch            -         41.0   \n",
       "                                       paged_adamw_8bit       -         38.0   \n",
       "        NVIDIA L4 x 4                  adamw_torch            -            -   \n",
       "                                       paged_adamw_8bit       -            -   \n",
       "        NVIDIA L4 x 8                  paged_adamw_8bit    85.0            -   \n",
       "\n",
       "                                                                        \\\n",
       "                                                                         \n",
       "model                                                   llama7 llama70   \n",
       "dataset hw                             optim                             \n",
       "r21     NVIDIA GeForce RTX 2080 Ti x 4 adamw_torch        26.0       -   \n",
       "                                       paged_adamw_8bit   27.0       -   \n",
       "        NVIDIA L4 x 4                  adamw_torch        17.0       -   \n",
       "                                       paged_adamw_8bit   18.0       -   \n",
       "        NVIDIA L4 x 8                  paged_adamw_8bit      -    44.0   \n",
       "\n",
       "                                                                              \\\n",
       "                                                        eval/f1_bi             \n",
       "model                                                      llama13    llama7   \n",
       "dataset hw                             optim                                   \n",
       "r21     NVIDIA GeForce RTX 2080 Ti x 4 adamw_torch        0.865225  0.863866   \n",
       "                                       paged_adamw_8bit   0.865672  0.860927   \n",
       "        NVIDIA L4 x 4                  adamw_torch               -  0.861953   \n",
       "                                       paged_adamw_8bit          -  0.868167   \n",
       "        NVIDIA L4 x 8                  paged_adamw_8bit          -         -   \n",
       "\n",
       "                                                                           \\\n",
       "                                                                  test_f1   \n",
       "model                                                     llama70 llama13   \n",
       "dataset hw                             optim                                \n",
       "r21     NVIDIA GeForce RTX 2080 Ti x 4 adamw_torch              -   0.814   \n",
       "                                       paged_adamw_8bit         -   0.821   \n",
       "        NVIDIA L4 x 4                  adamw_torch              -       -   \n",
       "                                       paged_adamw_8bit         -       -   \n",
       "        NVIDIA L4 x 8                  paged_adamw_8bit  0.843077       -   \n",
       "\n",
       "                                                                        \\\n",
       "                                                                         \n",
       "model                                                   llama7 llama70   \n",
       "dataset hw                             optim                             \n",
       "r21     NVIDIA GeForce RTX 2080 Ti x 4 adamw_torch       0.812       -   \n",
       "                                       paged_adamw_8bit  0.834       -   \n",
       "        NVIDIA L4 x 4                  adamw_torch           -       -   \n",
       "                                       paged_adamw_8bit  0.806       -   \n",
       "        NVIDIA L4 x 8                  paged_adamw_8bit      -       -   \n",
       "\n",
       "                                                                                \n",
       "                                                        time(h)                 \n",
       "model                                                   llama13 llama7 llama70  \n",
       "dataset hw                             optim                                    \n",
       "r21     NVIDIA GeForce RTX 2080 Ti x 4 adamw_torch          6.4   3.62       -  \n",
       "                                       paged_adamw_8bit    6.57   5.12       -  \n",
       "        NVIDIA L4 x 4                  adamw_torch            -   4.63       -  \n",
       "                                       paged_adamw_8bit       -   7.84       -  \n",
       "        NVIDIA L4 x 8                  paged_adamw_8bit       -      -   25.26  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piv = data.pivot_table(index=['dataset','hw', 'optim'], columns=['model'], values=['test_f1','eval/f1_bi','allocatedgbmax','allocatedmax','time(h)'], aggfunc=['mean'],fill_value='-')\n",
    "piv.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"12\" halign=\"left\">count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">allocatedgbmax</th>\n",
       "      <th colspan=\"3\" halign=\"left\">eval/f1_bi</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_f1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">time(h)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>llama13</th>\n",
       "      <th>llama7</th>\n",
       "      <th>llama70</th>\n",
       "      <th>llama13</th>\n",
       "      <th>llama7</th>\n",
       "      <th>llama70</th>\n",
       "      <th>llama13</th>\n",
       "      <th>llama7</th>\n",
       "      <th>llama70</th>\n",
       "      <th>llama13</th>\n",
       "      <th>llama7</th>\n",
       "      <th>llama70</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>hw</th>\n",
       "      <th>optim</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">r21</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">NVIDIA GeForce RTX 2080 Ti x 4</th>\n",
       "      <th>adamw_torch</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paged_adamw_8bit</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">NVIDIA L4 x 4</th>\n",
       "      <th>adamw_torch</th>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paged_adamw_8bit</th>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NVIDIA L4 x 8</th>\n",
       "      <th>paged_adamw_8bit</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 count         \\\n",
       "                                                        allocatedgbmax          \n",
       "model                                                          llama13 llama7   \n",
       "dataset hw                             optim                                    \n",
       "r21     NVIDIA GeForce RTX 2080 Ti x 4 adamw_torch                   1      1   \n",
       "                                       paged_adamw_8bit              1      1   \n",
       "        NVIDIA L4 x 4                  adamw_torch                   -      1   \n",
       "                                       paged_adamw_8bit              -      1   \n",
       "        NVIDIA L4 x 8                  paged_adamw_8bit              -      -   \n",
       "\n",
       "                                                                            \\\n",
       "                                                                eval/f1_bi   \n",
       "model                                                   llama70    llama13   \n",
       "dataset hw                             optim                                 \n",
       "r21     NVIDIA GeForce RTX 2080 Ti x 4 adamw_torch            -          1   \n",
       "                                       paged_adamw_8bit       -          1   \n",
       "        NVIDIA L4 x 4                  adamw_torch            -          -   \n",
       "                                       paged_adamw_8bit       -          -   \n",
       "        NVIDIA L4 x 8                  paged_adamw_8bit       1          -   \n",
       "\n",
       "                                                                        \\\n",
       "                                                                         \n",
       "model                                                   llama7 llama70   \n",
       "dataset hw                             optim                             \n",
       "r21     NVIDIA GeForce RTX 2080 Ti x 4 adamw_torch           1       -   \n",
       "                                       paged_adamw_8bit      1       -   \n",
       "        NVIDIA L4 x 4                  adamw_torch           1       -   \n",
       "                                       paged_adamw_8bit      1       -   \n",
       "        NVIDIA L4 x 8                  paged_adamw_8bit      -       1   \n",
       "\n",
       "                                                                        \\\n",
       "                                                        test_f1          \n",
       "model                                                   llama13 llama7   \n",
       "dataset hw                             optim                             \n",
       "r21     NVIDIA GeForce RTX 2080 Ti x 4 adamw_torch            1      1   \n",
       "                                       paged_adamw_8bit       1      1   \n",
       "        NVIDIA L4 x 4                  adamw_torch            -      0   \n",
       "                                       paged_adamw_8bit       -      1   \n",
       "        NVIDIA L4 x 8                  paged_adamw_8bit       -      -   \n",
       "\n",
       "                                                                         \\\n",
       "                                                                time(h)   \n",
       "model                                                   llama70 llama13   \n",
       "dataset hw                             optim                              \n",
       "r21     NVIDIA GeForce RTX 2080 Ti x 4 adamw_torch            -       1   \n",
       "                                       paged_adamw_8bit       -       1   \n",
       "        NVIDIA L4 x 4                  adamw_torch            -       -   \n",
       "                                       paged_adamw_8bit       -       -   \n",
       "        NVIDIA L4 x 8                  paged_adamw_8bit       0       -   \n",
       "\n",
       "                                                                        \n",
       "                                                                        \n",
       "model                                                   llama7 llama70  \n",
       "dataset hw                             optim                            \n",
       "r21     NVIDIA GeForce RTX 2080 Ti x 4 adamw_torch           1       -  \n",
       "                                       paged_adamw_8bit      1       -  \n",
       "        NVIDIA L4 x 4                  adamw_torch           1       -  \n",
       "                                       paged_adamw_8bit      1       -  \n",
       "        NVIDIA L4 x 8                  paged_adamw_8bit      -       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.pivot_table(index=['dataset','hw', 'optim'], columns=['model'], values=['test_f1','eval/f1_bi','allocatedgbmax','time(h)'], aggfunc=['count'],fill_value='-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_explore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
